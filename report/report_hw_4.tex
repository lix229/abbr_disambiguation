\documentclass{article}
\usepackage{hyperref}
\usepackage{array}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\usepackage{geometry}
\linespread{2}
\usepackage[dvipsnames]{xcolor}
\usepackage{amsmath}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=[RGB]{51,51,153},
    urlcolor=black
}
\usepackage{graphicx}
\graphicspath{ {./} }
\usepackage{xcolor}
\usepackage{listings}
\usepackage{xparse}

\NewDocumentCommand{\codeword}{v}{%
\texttt{\textcolor{RedViolet}{#1}}%
}



\author{Xiaohan Li}
\title{\huge{A Machine Learning Approach to Abbreviation Word Sense Disambiguation}\\
\LARGE{Homework 4 Report}}
\geometry{
    a4paper,
    total={170mm,257mm},
    }

\begin{document}
    \newgeometry{top=7cm, bottom = 3cm}
    \begin{titlepage}
        \maketitle    
    \end{titlepage}
    \newgeometry{margin=3cm}
    \tableofcontents
    \section{Literature Review}
    \section{Data Exploration and Preprocessing}
    \subsection{Data Exploration}
    Original data contains 75 different abbreviations, and each abbreviation comes with 500 samples with pre-annotations.\\
    In order to pick an abbreviation that could yield the most meaningful result, the following steps were taken:
    \begin{enumerate}
        \item Get number of senses for each abbreviation.\\
              One abbreviation with only 1 sense is dropped.
        \item Get standard deviation of the occurrence of each sense.
        \item The results are ranked by standard deviation from low to high, and then by number of senses from high to low.
    \end{enumerate}
    The rational is to pick the abbreviation with the most even distribution of senses, and the most number of senses.
    The final abbreviation chosen is \codeword{"CVA"}, which has 2 senses and the standard deviation of the occurrence of each sense is 39.6, resulting in a relatively even distribution of the senses.
    \subsection{Data Preprocessing}
    After careful investigation of the data. The following preprocessing steps were taken:
    \begin{enumerate}
        \item For each block of sample text provided, the text block are broken down into list of sentences using \codeword{'.'} as the delimiter. Sentences are then broken down into list of words using \codeword{' '} as the delimiter.
        \item Special characters are removed from the sentences. e.g. \codeword{'(', ')'}. If a word contains special characters, it is cut into 2 words, e.g. \codeword{'metacarpophalangeal(mp)'} would be cut into \codeword{'metacarpophalangeal'} and \codeword{'mp'}.
        \item sentences that do not contain the abbreviation are removed.
        \item If a text block contains 2 or more sentences that contain the abbreviation, the text block is split into corresponding number of text blocks, each containing only 1 sentence that contains the abbreviation.
        \item De-identified date and time in different formats are unifed into \codeword{'_%#DATE#%_'}, similarly for \codeword{'_%#ZIP#%_'}.
        \item After manual inspection, all 4 digits numbers in the data set are indeed year numbers, and are unifed into \codeword{'_%#DATE#%_'}.
    \end{enumerate}
    A total number 518 samples are generated after the preprocessing steps.
    \section{Methods}
    \subsection{Feature Extraction}
    Features explored in this project is based on \textit{n-gram} where \textit{n-1} is the window size of the \textit{n-gram}. Features are generated by combining the window size and the following feature types:
    \begin{enumerate}
        \item word only e.g. \codeword{'[word1, word2...]'}.
        \item word with direction e.g. \codeword{'[l-word1, l-word2..., r-word1, r-word2...]'}.
        \item word with direction and distance to abbreviation. e.g. \codeword{'[l-n-word1, l-(n-1)-word2..., r-1-word1, r-2-word2...]'}.
    \end{enumerate}
    $n \in (2, 3, 4, 5)$ as window sized are used in this project. Resulting a total number of $4 \times 3 = 12$ types of features.
    The final feature generated is all words combined with their respective frequencies as values. The labels are senses mapped into integers.
    \subsection{Feature Set Selection}
    The generated features are fed into a random forest classifier with hyperparameter tuning by grid search to find the best combination of features.
    The results are ranked by the F1 score of the classifier. 
    \subsubsection{Feature Set Exploration}
    The following graphs shows the impact of the window size given the feature type:\\
    \includegraphics[width=13.4cm]{./by_feature.png}\\
    The following graphs shows the impact of the feature type given the window size:\\
    \includegraphics[width=13.4cm]{./by_window_size.png}\\
    \subsubsection{Best Performing Feature Set}
    The best performing set of featrues a 3-gram with basic word features, with:
    \begin{itemize}
        \item Accuracy: 99.22 (0.94\%)
        \item F1 score: 0.98
        \item ROC AUC Score:  0.97
    \end{itemize}
    \subsection{Model Selection}
    Using the best performing feature set established in the previous section, the following models are explored and compared with 5 fold stratified cross validation and grid search hyperparameter tuning (20\% out of bag data as test set):\\\\
    \begin{tabular}{|M{4cm}||M{4cm}|M{2cm}|M{2cm}|}
        \hline
        \textbf{Classifier} & \textbf{Accuracy \& STD} & \textbf{F1 Score} & \textbf{ROC AUC}\\
        \hline
        Random Forest & 99.22\% (0.88\%) & 0.98 & 0.97  \\
        \hline
        Gradient Boosting & 99.21\% (0.94\%) & 0.98 & 0.97  \\
        \hline
        Ada Boost & 97.07\% (1.99\%) & 0.98 & 0.97  \\
        \hline
        Gaussian Naive Bayes & 88.89\% (3.00\%) & 0.93 & 0.92  \\
        \hline
    \end{tabular}\\\\
    As the table suggested, random forest classifier is the best performing model with a slight edge over gradient boosting classifier.\\
    \section{Error Analysis}
    \subsection{Classification Statistics}
    Classification Report:\\\\
    \begin{tabular}{|M{3cm}||M{2.5cm}|M{2.5cm}|M{2.5cm}|M{2.5cm}|}
        \hline
            &  precision  &  recall  &f1-score  & support\\
        \hline
           0    &   0.97    &  0.95   &   0.96    &    37\\
        \hline
           1    &   0.97    &  0.99  &    0.98     &   67\\
        \hline
        accuracy     &        &       & 0.97   &    104\\
        \hline
        macro avg   &    0.97   &   0.97   &   0.97  &     104\\
        \hline
        weighted avg    &   0.97    &  0.97   &   0.97  &     104   \\
        \hline
    \end{tabular}\\\\
    The confusion matrix:\\\\
    \begin{tabular}{|M{3cm}|M{3cm}|M{3cm}|}
        \hline
        & Actual 0 & Actual 1\\
        \hline
        Predicted 0 & 35 & 2\\
        \hline
        Predicted 1 & 1 & 66\\
        \hline
    \end{tabular}\\\\
    Since the data set is somewhat imbalanced, the performance when predicting the minority class is expected to be lower than the majority class.\\
    The following table shows the data ID, actual and predicted labels of the 3 misclassified samples and the original n-gram:\\\\
    \begin{tabular}{|M{2cm}|M{2cm}|M{2cm}|M{6cm}|}
        \hline
        Data ID & Actual & Predicted & Original 3-gram\\
        \hline
        249 & 1 & 0 & ['No', 'CVA', 'tenderness']\\
        \hline
        42 & 0 & 1&['the', 'left', 'CVA', 'costal', 'margin']\\
        \hline
        69 & 0 & 1 & ['Spine', 'and', 'CVA', 'did', 'not']\\
        \hline
    \end{tabular}\\
    \subsection{Error Analysis}
    \begin{itemize}
        \item ID\_249: The sentence containing the abbreviation is only 3 word long including the abbreviation itself, and the sentence appears evenly distributed across different senses. Thus the model is unable to make a good Classification.
        \item ID\_42: After examining the data and doing some preliminary research, we found that the phrase \codeword{'CVA costal margin'} is not a commonly used combination of words since the \textit{CVA} is formed by the 12th rib and the spine while the \textit{costal margin} is an arch formed by the medical margin of the seventh rib to the tenth rib. Thus the text does not match the usual pattern of the abbreviation well enough to be classified correctly.
        \item ID\_69: The sentence containing the abbreviation is in a negative context, while 3 out of 4 words in the 3-gram are \codeword{'and'}, \codeword{'did'} and \codeword{'not'}. which are common words that are not specific to any sense. Thus the model is unable to make a good classification.
    \end{itemize}
    \section{Results and Discussions}
    

\end{document}