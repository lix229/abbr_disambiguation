\documentclass{article}
\usepackage{hyperref}
\usepackage{array}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\usepackage{geometry}
\linespread{2}
\usepackage{amsmath}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=[RGB]{51,51,153},
    urlcolor=black
}
\usepackage{graphicx}
\graphicspath{ {./} }
\usepackage{xcolor}
\usepackage{listings}
\usepackage{xparse}

\NewDocumentCommand{\codeword}{v}{%
\texttt{\textcolor{blue}{#1}}%
}



\author{Xiaohan Li}
\title{\huge{A Machine Learning Approach to Abbreviation Word Sense Disambiguation}\\
\LARGE{Homework 4 Report}}
\geometry{
    a4paper,
    total={170mm,257mm},
    }

\begin{document}
    \newgeometry{top=7cm, bottom = 3cm}
    \begin{titlepage}
        \maketitle    
    \end{titlepage}
    \newgeometry{margin=3cm}
    \tableofcontents
    \section{Literature Review}
    \section{Data Exploration and Preprocessing}
    \subsection{Data Exploration}
    Original data contains 75 different abbreviations, and each abbreviation comes with 500 samples with pre-annotations.\\
    In order to pick an abbreviation that could yield the most meaningful result, the following steps were taken:
    \begin{enumerate}
        \item Get number of senses for each abbreviation.\\
              One abbreviation with only 1 sense is dropped.
        \item Get standard deviation of the occurrence of each sense.
        \item The results are ranked by standard deviation from low to high, and then by number of senses from high to low.
    \end{enumerate}
    The rational is to pick the abbreviation with the most even distribution of senses, and the most number of senses.
    The final abbreviation chosen is \codeword{"CVA"}, which has 2 senses and the standard deviation of the occurrence of each sense is 39.6, resulting in a relatively even distribution of the senses.
    \subsection{Data Preprocessing}
    After careful investigation of the data. The following preprocessing steps were taken:
    \begin{enumerate}
        \item For each block of sample text provided, the text block are broken down into list of sentences using \codeword{'.'} as the delimiter. Sentences are then broken down into list of words using \codeword{' '} as the delimiter.
        \item Special characters are removed from the sentences. e.g. \codeword{'(', ')'}. If a word contains special characters, it is cut into 2 words, e.g. \codeword{'metacarpophalangeal(mp)'} would be cut into \codeword{'metacarpophalangeal'} and \codeword{'mp'}.
        \item sentences that do not contain the abbreviation are removed.
        \item If a text block contains 2 or more sentences that contain the abbreviation, the text block is split into corresponding number of text blocks, each containing only 1 sentence that contains the abbreviation.
        \item De-identified date and time in different formats are unifed into \codeword{'_%#DATE#%_'}, similarly for \codeword{'_%#ZIP#%_'}.
    \end{enumerate}
    A total number 518 samples are generated after the preprocessing steps.
    \section{Methods}
    \subsection{Feature Extraction}
    Features explored in this project is based on \textit{n-gram} where \textit{n-1} is the window size of the \textit{n-gram}. Features are generated by combining the window size and the following feature types:
    \begin{enumerate}
        \item word only e.g. \codeword{'[word1, word2...]'}.
        \item word with direction e.g. \codeword{'[l-word1, l-word2..., r-word1, r-word2...]'}.
        \item word with direction and distance to abbreviation. e.g. \codeword{'[l-n-word1, l-(n-1)-word2..., r-1-word1, r-2-word2...]'}.
    \end{enumerate}
    $n \in (2, 3, 4, 5)$ as window sized are used in this project. Resulting a total number of $4 \times 3 = 12$ types of features.
    The final feature generated is all words combined with their respective frequencies as values. The labels are senses mapped into integers.
    \subsection{Feature Set Selection}
    The generated features are fed into a random forest classifier with hyperparameter tuning by grid search to find the best combination of features.
    The results are ranked by the F1 score of the classifier. The best performing set of featrues a 3-gram with basic word features, with:
    \begin{itemize}
        \item Accuracy: 99.22 (0.94\%)
        \item F1 score: 0.98
        \item ROC AUC Score:  0.97
    \end{itemize}
    \subsection{Model Selection}
\end{document}